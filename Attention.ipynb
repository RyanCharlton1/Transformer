{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "RNNs will favour more recent inputs as the proportion of the output that an earlier recurred output is responsible for will exponentially decay passed through a balanced set of weights repeatedly. Attention solves this problem by directly considering every symbol in a sequence.\n",
    "\n",
    "Attention is modeled as a database of key value pairs. Each query is matched to every key and a similarity score computed. The similarity scores are used as softmax input to generate a weight for a each key and the output is the weighted sum of all values for each query. The keys, queries and values have their own learned latent spaces.\n",
    "\n",
    "Attention also is Parallelisable, unlike RNNs where the output of a pass is used in the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.40355304 -0.66996238  1.89421245]\n",
      " [ 7.64042136 -1.92736712  5.0570968 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 6.69904830e-02, -9.31449150e-01],\n",
       "        [ 1.56768043e-26, -2.24831539e-25]]),\n",
       " array([[-7.30964179e-01, -5.32471819e-01],\n",
       "        [-1.17104623e-25, -8.46764949e-26]]),\n",
       " array([[ 1.27416367,  3.89954714],\n",
       "        [ 4.55091222, 11.70034358]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "class Attention:\n",
    "    # d_K: dimension of key and query input\n",
    "    # d_V: dimension of value input\n",
    "    # d_L: dimension of latent spaces\n",
    "    def __init__(self, d_K, d_V, d_L, mask=None):\n",
    "        self.d_K = d_K\n",
    "        self.d_V = d_V\n",
    "        self.d_L = d_L\n",
    "\n",
    "        self.mask = mask\n",
    "\n",
    "        # glorot initialization limits\n",
    "        limit = np.sqrt(6 / (d_K + d_L))\n",
    "\n",
    "        # weight matrices for query, key, and value latent spaces\n",
    "        self.w_Q = np.random.uniform(-limit, limit, size=(d_K, d_L))\n",
    "        self.w_K = np.random.uniform(-limit, limit, size=(d_K, d_L))\n",
    "        self.w_V = np.random.uniform(-limit, limit, size=(d_V, d_L))\n",
    "\n",
    "    # Q: query input\n",
    "    # K: key input\n",
    "    # V: value input\n",
    "    # for self-attention Q K and V are the same \n",
    "    def calc_attention(self, Q, K, V):\n",
    "        # hold inputs for gradient calculations\n",
    "        self.Q = Q\n",
    "        self.K = K\n",
    "        self.V = V\n",
    "\n",
    "        # calculate latent space representations\n",
    "        self.lQ = Q @ self.w_Q\n",
    "        self.lK = K @ self.w_K\n",
    "        self.lV = V @ self.w_V\n",
    "\n",
    "        # calculate similarity by matching each query to every key\n",
    "        similarity = self.lQ @ self.lK.T\n",
    "\n",
    "        # scale similarity\n",
    "        similarity = similarity / np.sqrt(self.d_L)\n",
    "\n",
    "        if self.mask is not None:\n",
    "            similarity = similarity * self.mask\n",
    "        \n",
    "        # calculate weights\n",
    "        self.w_A = softmax(similarity, axis=1)\n",
    "\n",
    "        # apply weights to value vectors and sum results\n",
    "        return np.sum(self.w_A[:, :, np.newaxis] * self.lV[np.newaxis, :, :], axis=1)\n",
    "    \n",
    "    # g_A: gradient at attention output\n",
    "    def calc_gradients(self, g_A):\n",
    "        # gradient at values in latent space\n",
    "        g_lV = self.w_A.T @ g_A\n",
    "        # gradient at final weights, post softmax\n",
    "        g_wA = g_A @ self.lV.T\n",
    "\n",
    "        # gradient of values input pre latent space transformation\n",
    "        g_V = g_lV @ self.w_V.T \n",
    "        # gradient of latent space transformation\n",
    "        self.g_w_lV = self.V.T @ g_lV\n",
    "\n",
    "        # gradient of similarity scores, pre softmax \n",
    "\n",
    "        # softmax derivative is jacobian matrix since all elems of input\n",
    "        # vector effect all elems of output vector. each attention output\n",
    "        # has its own softmax output, so multiple jacobian matrices\n",
    "        jacobs = np.array([np.outer(row, row) for row in self.w_A])\n",
    "        jacobs *= -1\n",
    "        jacobs += np.array([np.diag(row) for row in self.w_A])\n",
    "        \n",
    "        g_sim = jacobs @ g_wA[:, :, np.newaxis]\n",
    "        g_sim = g_sim.squeeze()\n",
    "        \n",
    "        # mask is its own gradient\n",
    "        if self.mask is not None:\n",
    "            g_sim = g_sim * self.mask\n",
    "\n",
    "        # gradient of scaling\n",
    "        g_sim = g_sim / np.sqrt(self.d_L)\n",
    "        \n",
    "        # gradient at latent space\n",
    "        g_lQ = g_sim @ self.lK\n",
    "        g_lK = g_sim.T @ self.lQ\n",
    "\n",
    "        # gradients pre latent space transformation\n",
    "        g_Q = g_lQ @ self.w_Q.T\n",
    "        g_K = g_lK @ self.w_K.T\n",
    "\n",
    "        # gradients of latent space transformations\n",
    "        self.g_w_Q = self.Q.T @ g_lQ\n",
    "        self.g_w_K = self.K.T @ g_lK\n",
    "\n",
    "        return (g_Q, g_K, g_V)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
