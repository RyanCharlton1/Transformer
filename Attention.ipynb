{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "RNNs will favour more recent inputs as the proportion of the output that an earlier recurred output is responsible for will exponentially decay passed through a balanced set of weights repeatedly. Attention solves this problem by directly considering every symbol in a sequence.\n",
    "\n",
    "Attention is modeled as a database of key value pairs. Each query is matched to every key and a similarity score computed. The similarity scores are used as softmax input to generate a weight for a each key and the output is the weighted sum of all values for each query. The keys, queries and values have their own learned latent spaces.\n",
    "\n",
    "Attention also is Parallelisable, unlike RNNs where the output of a pass is used in the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "class Attention:\n",
    "    # d_K: dimension of key and query input\n",
    "    # d_V: dimension of value input\n",
    "    # d_L: dimension of latent spaces\n",
    "    def __init__(self, d_K, d_V, d_L):\n",
    "        self.d_K = d_K\n",
    "        self.d_V = d_V\n",
    "        self.d_L = d_L\n",
    "\n",
    "        # glorot initialization limits\n",
    "        limit = np.sqrt(6 / (d_K + d_L))\n",
    "\n",
    "        # weight matrices for query, key, and value\n",
    "        self.W_Q = np.random.uniform(-limit, limit, size=(d_L, d_K))\n",
    "        self.W_K = np.random.uniform(-limit, limit, size=(d_L, d_K))\n",
    "        self.W_V = np.random.uniform(-limit, limit, size=(d_L, d_V))\n",
    "\n",
    "    # Q: query input\n",
    "    # K: key input\n",
    "    # V: value input\n",
    "    # for self-attention Q K and V are the same \n",
    "    def calc_attention(self, Q, K, V):\n",
    "        # calculate latent space representations\n",
    "        self.l_Q = Q @ self.W_Q.T\n",
    "        self.l_K = K @ self.W_K.T\n",
    "        self.l_V = V @ self.W_V.T\n",
    "\n",
    "        # calculate similarity by matching each query to every key\n",
    "        similarity = self.l_Q @ self.l_K.T\n",
    "\n",
    "        # scale similarity\n",
    "        similarity = similarity / np.sqrt(self.d_L)\n",
    "        \n",
    "        # calculate weights\n",
    "        weights = softmax(similarity, axis=1)\n",
    "\n",
    "        # apply weights to values and sum results\n",
    "        return np.sum(weights[:, :, np.newaxis] * self.l_V[np.newaxis, :, :], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
